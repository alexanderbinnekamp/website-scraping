{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da64c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "For this assignment I have chosen to scrape the archive website of nu.nl. In step 1 I import all the essential libraries, namely\n",
    "requests, beautifulsoup and pandas. In step 2 I import the textual data from the website. In step 3 I am searching the HTML string \n",
    "for specific HTML data using the lxml parser. I first do this only for the header article. The particular data I am looking for\n",
    "is the title of the article and the link referring to the article. In step 4 I am following the link I have found earlier in \n",
    "order to extract also the publication date. I use the Try and Except method because the website uses two different formats, \n",
    "namely a format for a text article (try) and a format for a video article (except). In step 5 I am repeating the previous steps\n",
    "for the articles below the header article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68894ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05444453",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': ['Apple for ever', 'Lopend', 'CIDI niet blij met link NVU', 'Airfood', 'Romeinse schat in Vollenhoofse grond', 'Actieplan moet toerist naar Schouwen lokken', 'Afroepauto scoort in Apeldoorn nog niet', 'New Yorkse joden gaan gewapend patrouilleren', 'Man opgepakt die radioactieve aanslag wilde plegen', 'Papierloos kantoor', 'Spinvis', \"'Al-Qaeda waarschuwt VS voor nieuwe aanslagen'\", 'Amerikanen doden Afghaanse bondgenoten', 'Ruim 800.000 vluchtelingen teruggekeerd in Afghanistan', 'FIFA treedt streng op tegen schwalbes', 'Rode kaart voor @Home', 'Ruim de helft van Kamerleden komt niet terug', 'SP en GroenLinks willen niet regeren met LPF', 'Amerika vreest nieuwe terreurgolf', ' Balkenende wil coalitie met LPF en VVD'], 'Date_of_publication': ['13 augustus 2002 13:03', '08 augustus 2002 12:22', '01 augustus 2002 12:35', '30 juli 2002 12:38', '02 februari 2002 10:27', '31 januari 2002 14:26', '01 februari 2002 21:15', '10 juni 2002 17:37', '10 juni 2002 17:31', '10 juni 2002 13:55', '03 juni 2002 14:35', '02 juni 2002 09:53', '01 juni 2002 11:21', '30 mei 2002 20:05', '30 mei 2002 14:29', '27 mei 2002 13:54', '22 mei 2002 14:56', '22 mei 2002 11:55', '22 mei 2002 07:04', '21 mei 2002 15:19'], 'Link': ['https://nu.nl/archief/4756/apple-for-ever.html', 'https://nu.nl/archief/5588/lopend.html', 'https://nu.nl/archief/57324/cidi-niet-blij-met-link-nvu.html', 'https://nu.nl/archief/56880/airfood.html', 'https://nu.nl/archief/23168/romeinse-schat-in-vollenhoofse-grond.html', 'https://nu.nl/archief/22740/actieplan-moet-toerist-naar-schouwen-lokken.html', 'https://nu.nl/archief/23120/afroepauto-scoort-in-apeldoorn-nog-niet.html', 'https://nu.nl/archief/47672/new-yorkse-joden-gaan-gewapend-patrouilleren.html', 'https://nu.nl/archief/47668/man-opgepakt-die-radioactieve-aanslag-wilde-plegen.html', 'https://nu.nl/archief/47620/papierloos-kantoor.html', 'https://nu.nl/archief/46328/spinvis.html', 'https://nu.nl/archief/46120/al-qaeda-waarschuwt-vs-voor-nieuwe-aanslagen.html', 'https://nu.nl/archief/46036/amerikanen-doden-afghaanse-bondgenoten.html', 'https://nu.nl/archief/45744/ruim-800000-vluchtelingen-teruggekeerd-in-afghanistan.html', 'https://nu.nl/archief/45644/fifa-treedt-streng-op-tegen-schwalbes.html', 'https://nu.nl/archief/44944/rode-kaart-voor-home.html', 'https://nu.nl/archief/44132/ruim-de-helft-van-kamerleden-komt-niet-terug.html', 'https://nu.nl/archief/44084/sp-en-groenlinks-willen-niet-regeren-met-lpf.html', 'https://nu.nl/archief/43976/amerika-vreest-nieuwe-terreurgolf.html', 'https://nu.nl/archief/43896/-balkenende-wil-coalitie-met-lpf-en-vvd.html'], 'Date_stored': datetime.date(2022, 1, 5)}\n"
     ]
    }
   ],
   "source": [
    "dictionary = {'Title' : [], 'Date_of_publication' : [], 'Link' : [], 'Date_stored' : date.today()}\n",
    "\n",
    "html_text = requests.get('https://www.nu.nl/archief').text\n",
    "soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "head_article = soup.find('div', class_ = 'block headline None')\n",
    "dictionary['Title'].append(head_article.h1.text)\n",
    "link1 = head_article.a['href']\n",
    "link1 = 'https://nu.nl' + link1\n",
    "dictionary['Link'].append(link1)\n",
    "\n",
    "try: \n",
    "    html_text2 = requests.get(link1).text\n",
    "    soup2 = BeautifulSoup(html_text2, 'lxml')\n",
    "    metadata = soup2.find('div', class_ = 'metadata')\n",
    "    date_of_publication = metadata.span.text\n",
    "    dictionary['Date_of_publication'].append(date_of_publication)\n",
    "        \n",
    "except:\n",
    "    html_text2 = requests.get(link).text\n",
    "    soup2 = BeautifulSoup(html_text2, 'lxml')\n",
    "    metadata = soup2.find('p', class_ = 'video-block__published-at body-style body-style--s0').text\n",
    "    metadata = metadata.replace('Gepubliceerd op ', '')\n",
    "    dictionary['Date_of_publication'].append(metadata)\n",
    "\n",
    "\n",
    "news_articles = soup.find_all('li', class_ = 'list__item list__item--thumb')\n",
    "for news_article in news_articles: \n",
    "    dictionary['Title'].append(news_article.span['title'])\n",
    "    \n",
    "    link = news_article.a['href']\n",
    "    link = 'https://nu.nl' + link\n",
    "    dictionary['Link'].append(link)\n",
    "    \n",
    "    try: \n",
    "        html_text2 = requests.get(link).text\n",
    "        soup2 = BeautifulSoup(html_text2, 'lxml')\n",
    "        metadata = soup2.find('div', class_ = 'metadata')\n",
    "        date_of_publication = metadata.span.text\n",
    "        dictionary['Date_of_publication'].append(date_of_publication)\n",
    "\n",
    "        \n",
    "    except:\n",
    "        html_text2 = requests.get(link).text\n",
    "        soup2 = BeautifulSoup(html_text2, 'lxml')\n",
    "        metadata = soup2.find('p', class_ = 'video-block__published-at body-style body-style--s0').text\n",
    "        date_of_publication = metadata.replace('Gepubliceerd op ', '')\n",
    "        dictionary['Date_of_publication'].append(date_of_publication)\n",
    "    \n",
    "print(dictionary)\n",
    "\n",
    "dataframe = pd.DataFrame(dictionary)\n",
    "dataframe.to_csv('C:/Users/alexa/OneDrive/Documenten/Collecting Data & Tools and Methods/nu.nl archive/file.csv')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963ff335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
